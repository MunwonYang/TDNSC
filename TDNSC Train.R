#### This function that computes TDNSC for tensor data.
# X: Array of n list of predictor data, which each dimension is p1*p2*...*pM
# Y: Class label corresponding with each X tensor data
# lambda: A sequence of user-specified lambda values. Default is NULL
# nlambda: Number of lambda values when the sequence of threshold parameter for TDNSC is not assigned. Default is 20

library(rTensor)
library(tensr)
library(expm)
source("~/Basic Setting.R")

TDNSC.train <- function(X,Y,lambda = NULL, nlambda = 20){
  dimen = dim(X[[1]]) # dimension of dataset
  nvars = prod(dimen) # number of variables
  n = length(Y) # number of dataset
  K = length(unique(Y)) # number of class label
  check = 0
  for(i in 1:n){
    if(sum(dimen == dim(X[[i]])) == length(dimen)){
      check = check + 1
    }
  }
  
  if(check == n){
    # Calculate the sample grand mean tensor
    hatmu = array(NA,c(n,nvars)) 
    for(i in 1:n){
      hatmu[i,] <- array(X[[i]],nvars)
    }
    hatmu <- array(apply(hatmu,2,mean),dimen)
    
    # Calculate the number of sample for each class
    N = array(list(),K)
    for (i in 1:K){
      N[[i]] = sum(Y == i)
    }
    
    # Calculate the proportion of samples in each class
    p = array(list(),K)
    for(i in 1:K){
      p[[i]] = N[[i]]/n
    }
    
    location <- array(list(),K)
    for(i in 1:n){
      for(k in 1:K){
        if(Y[i] == k){
          location[[k]] <- c(location[[k]],i)
        }
      }
    }
    
    # Estimated each class tensor mean
    sample_mean = array(list(),K)
    samples <- array(NA,c(n,nvars))
    for(i in 1:n){
      samples[i,] <- array(X[[i]],nvars)
    }
    
    for(k in 1:K){
      sample_mean[[k]] <- array(apply(samples[location[[k]],],2,mean),dimen)
    }
    
    P = array(list(),length(dimen))
    for (i in 1:length(dimen)){
      P[[i]] = prod(dimen) / dimen[i]
    }
    
    # Estimated covariance matrix on each mode. "Sigma" is a list with ith element being covariance matrix on ith mode.
    Sigma <- array(list(),c(length(dimen),K))
    
    for(i in 1:length(dimen)){
      for(k in  1:K){
        Sigma[[i]][[k]] <- array(0,c(dimen[i],dimen[i]))
      }
    }
    
    for(i in 1:length(dimen)){
      for(k in 1:K){
        for(j in 1:n){
          if(Y[j] == k){
            Sigma[[i]][[k]] <- Sigma[[i]][[k]] +  unfold(X[[j]] - sample_mean[[k]],i) %*% t(unfold(X[[j]] - sample_mean[[k]],i))
          }
        }
        Sigma[[i]][[k]] <- 1 / ((N[[k]] - 1) * P[[i]]) * Sigma[[i]][[k]]
      }
    }
    
    sigma <- array(list(),length(dimen))
    for(i in 1:length(dimen)){
      sigma[[i]] <- array(0,c(dimen[i],dimen[i]))
    }
    
    for(i in 1:length(dimen)){
      for(k in 1:K){
        sigma[[i]] <- sigma[[i]] + (N[[k]] - 1) / (n-K) * Sigma[[i]][[k]]
      }
    }
    
    # Estimated inverse covariance matrix on each mode.
    invSigma = array(list(),length(dimen))
    for(i in 1:length(dimen)){
      invSigma[[i]] = sqrtm(chol2inv(chol(sigma[[i]])))
    }
    
    # Decorrelation of each tensor data
    U = array(list(),n)
    for(i in 1:n){
      U[[i]] = atrans(X[[i]] - hatmu,invSigma)
    }
    
    # Estimated sample class centroid.
    paraU = array(list(),K)
    sampleU <- array(NA,c(n,nvars))
    for(i in 1:n){
      sampleU[i,] <- array(U[[i]],nvars)
    }
    
    for(k in 1:K){
      paraU[[k]] <- array(apply(sampleU[location[[k]],],2,mean),dimen)
    }
    
    # If lambda is not specified, lambda sequence is generated by nlambda.
    if(is.null(lambda)){
      maxval = 0
      for(i in 1:K){
        maxval = max(maxval,max(abs(as.numeric(paraU[[i]]))))
      }
      lambda = seq(0,maxval,maxval/nlambda)[-1]
    }
    else{
      nlambda = length(lambda)
    }
    
    # Calculation of each class shrunken centroid by different soft-thresholding value.
    predU = array(list(),nlambda)
    
    for(i in 1:nlambda){
      for (k in 1:K){
        predU[[i]][[k]] = shrink(paraU[[k]],lambda[i])
      }
    }
    
    # Calculation of nonzero elements of each class shrunken centroid by different soft-thresholding value. 
    nonzero <- array(NA,c(nlambda,K))
    for(i in 1:nlambda){
      for(k in 1:K){
        nonzero[i,k] = sum( predU[[i]][[k]] != 0)
      }
    }
    
    # Estimation of class label by using above sample statistic.
    Yres = matrix(0,n,nlambda)
    for(i in 1:nlambda){
      for(j in 1:n){
        a = array(NA,K)
        for(k in 1:K){
          a[k] = sum((U[[j]] - predU[[i]][[k]])*(U[[j]] - predU[[i]][[k]])) - 2 * log(p[[k]])
        }
        Yres[j,i] = which.min(a)
      }
    }
    
    # Calculation of error rate based on different soft-thresholding value.
    wrong = array(NA, nlambda)
    for(i in 1:nlambda){
      wrong[i] = sum(Yres[,i] != Y) / n * 100
    }
    
    obj <- list(prob = p, 
                lambda = lambda, predU = predU, 
                hatmu = hatmu, Sigma = sigma, invSigma = invSigma, 
                Yhat = Yres, error = wrong, nonzero = nonzero)
    class(obj) <- "TDNSC"
    obj
  }
  
  else{
    stop("Data Dimension doesn't match")
  }
}
